{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_stations = {\n",
    "    'Tarrant': ['DFW', 'GKY', 'FTW', 'AFW'],\n",
    "    'McLennan': ['ACT', 'PWG'],\n",
    "    'Montague': ['0F2'],\n",
    "    'Stephens': ['BKD'],\n",
    "    'Wise': ['XBP', 'LUD'],\n",
    "    'Johnson': ['CPT'],\n",
    "    'Comanche': ['MKN'],\n",
    "    'Navarro': ['CRS'],\n",
    "    'Dallas': ['DAL', 'RBD'],\n",
    "    'Denton': ['DTO'],\n",
    "    'Cooke': ['GLE'],\n",
    "    'Coryell': ['GOP'],\n",
    "    'Young': ['RPH'],\n",
    "    'Hood': ['GDJ'],\n",
    "    'Hunt': ['GVT'],\n",
    "    'Hamilton': ['MNZ'],\n",
    "    'Hill': ['INJ'],\n",
    "    'Collin': ['TKI'],\n",
    "    'Palo Pinto': ['MWL'],\n",
    "    'Grayson': ['GYI'],\n",
    "    'Erath': ['SEP'],\n",
    "    'Kaufman': ['TRL'],\n",
    "    'Ellis': ['JWY'],\n",
    "    'Eastland': ['BKD', 'MKN'],\n",
    "    'Jack': ['RPH', 'XBP', 'LUD'],\n",
    "    'Parker': ['MWL', 'GDJ', 'DFW', 'GKY', 'FTW', 'AFW', 'DTO'],\n",
    "    'Rockwall': ['DAL', 'RBD', 'TKI', 'GVT', 'TRL'],\n",
    "    'Somervell': ['SEP', 'GDJ', 'CPT'],\n",
    "    'Bosque': ['MNZ', 'INJ', 'ACT', 'PWG', 'GOP']\n",
    "}\n",
    "\n",
    "station_counties = {}\n",
    "for county, stations in county_stations.items():\n",
    "    for station in stations:\n",
    "        if station not in station_counties:\n",
    "            station_counties[station] = []\n",
    "        station_counties[station].append(county)\n",
    "station_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "selected_dfs = []\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "stations_dir = r\"..\\data\"\n",
    "for file in os.listdir(stations_dir):\n",
    "    station = file[:3]\n",
    "\n",
    "    if station not in station_counties:\n",
    "        continue\n",
    "\n",
    "    print(station)\n",
    "\n",
    "    path = os.path.join(stations_dir, file)\n",
    "    station_df = pd.read_csv(path)\n",
    "\n",
    "    #print(station_df.head(3))\n",
    "\n",
    "    selected = station_df[['Date', 'Avg Temp', 'Max Temp', 'Min Temp', 'Total Precip', 'Total Snow', 'Avg Wind Speed', 'Max Wind Speed']]\n",
    "\n",
    "    selected_cols = selected.columns.drop('Date')\n",
    "    selected[selected_cols] = selected[selected_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    selected['Date'] = pd.to_datetime(selected['Date'])\n",
    "\n",
    "    selected = selected[selected['Max Wind Speed'] < 100] # correct for some error where the max wind speed is swapped with the max wind direction\n",
    "    selected.dropna(subset=['Avg Temp', 'Max Temp', 'Min Temp'], inplace=True) # rows where these are NaN are likely missing all values anyway\n",
    "    selected.fillna(0, inplace=True)\n",
    "\n",
    "    selected.rename({'Avg Temp': f'{station}_avg_temp', \n",
    "                     'Max Temp': f'{station}_max_temp', \n",
    "                     'Min Temp': f'{station}_min_temp', \n",
    "                     'Total Precip': f'{station}_precip', \n",
    "                     'Total Snow': f'{station}_snow', \n",
    "                     'Avg Wind Speed': f'{station}_avg_wind', \n",
    "                     'Max Wind Speed': f'{station}_max_wind'}, axis=1, inplace=True)\n",
    "    \n",
    "    selected_dfs.append(selected)\n",
    "\n",
    "valid_dates = []\n",
    "\n",
    "for df in selected_dfs:\n",
    "    valid_dates.extend(df['Date'].values)\n",
    "\n",
    "unique_dates = sorted(list(set(valid_dates)))\n",
    "\n",
    "total = pd.DataFrame({'Date': unique_dates})\n",
    "\n",
    "for df in selected_dfs:\n",
    "    total = pd.merge(total, df, on='Date', how='left')\n",
    "\n",
    "total.interpolate(method='pad', inplace=True) # kind of shit method but does allow for the rest of the process to happen\n",
    "\n",
    "#total.to_csv('all_variables2.csv', index=False)\n",
    "total = total.dropna()#.to_csv('all_variables_full2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total = pd.read_csv('all_variables_full2.csv')\n",
    "county_total = pd.DataFrame()\n",
    "county_total['Date'] = total['Date']\n",
    "\n",
    "for county, stations in county_stations.items():\n",
    "    county_avg_columns = [f'{county}_avg_temp', f'{county}_max_temp', f'{county}_min_temp', \n",
    "                          f'{county}_precip', f'{county}_snow', f'{county}_avg_wind', f'{county}_max_wind']\n",
    "    station_columns = lambda station: [f'{station}_avg_temp', f'{station}_max_temp', f'{station}_min_temp', \n",
    "                          f'{station}_precip', f'{station}_snow', f'{station}_avg_wind', f'{station}_max_wind']\n",
    "    columns_to_average = {}\n",
    "    for station in stations:\n",
    "        cols = station_columns(station)\n",
    "        for i in range(len(county_avg_columns)):\n",
    "            if county_avg_columns[i] not in columns_to_average:\n",
    "                columns_to_average[county_avg_columns[i]] = []\n",
    "            columns_to_average[county_avg_columns[i]].append(cols[i])\n",
    "    \n",
    "    for col in county_avg_columns:\n",
    "        county_total[col] = total[columns_to_average[col]].mean(axis=1).round(3)\n",
    "\n",
    "county_total.to_csv('county_variables.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
